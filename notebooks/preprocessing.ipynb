{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5QU4YD7VAKS"
      },
      "source": [
        "# Instalações e imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSfulNm-VAKS",
        "outputId": "73d3e152-4863-4198-c7df-1252497eb2be"
      },
      "outputs": [],
      "source": [
        "%pip install shap\n",
        "%pip install xgboost\n",
        "%pip install mmh3\n",
        "%pip install xgboost\n",
        "# %pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RuLZU2geVAKT"
      },
      "outputs": [],
      "source": [
        "import hashlib\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "import mmh3\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import shap\n",
        "import tensorflow as tf\n",
        "import xgboost as xgb\n",
        "from IPython.core.display import HTML, display\n",
        "from sklearn.decomposition import PCA, TruncatedSVD\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import (\n",
        "  LabelEncoder,\n",
        "  MinMaxScaler,\n",
        "  OneHotEncoder,\n",
        "  StandardScaler\n",
        ")\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras import models, layers, regularizers, callbacks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBu-roc4VAKU"
      },
      "source": [
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3X0OHf-VAKU"
      },
      "source": [
        "# Em Comum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWZjbsJiVAKU"
      },
      "source": [
        "## Descrição do trabalho\n",
        "Este notebook realiza um experimento preliminar de contruir um modelo preditivo de regressão para valores de medicamentos a partir de uma base de dados que contém receitas médicas para medicamentos manipulados.\n",
        "Os campos da base de dados original são:\n",
        "- descricao - contém uma relação de principios ativos, dosagens e quantificações de dosagens, quantidade e descrição das formas farmacêuticas de medicamentos incluídos na formulação\n",
        "- criado - tem a data/hora da criação da receita, possivelmente quando ela entrou no sistema da farmácia\n",
        "- qtdInsumos - contém o número de insumos (chamados neste trabalho de princípio ativo) constantes na formulação prescrita e descrita na descrição\n",
        "- calculado - contém o valor calculado (não sabemos se automaticamente no cadastro ou pela atendente que cadastrou)\n",
        "- correto - contém o valor correto da medicação\n",
        "\n",
        "Ao longo do notebook o experimento será descrito"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2J2iUpAzVAKU"
      },
      "outputs": [],
      "source": [
        "# carregamos o dataframe com as informações das receitas\n",
        "df = pd.read_csv('./input/dados_preco.csv',sep=',')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwFzV0ARVAKV"
      },
      "source": [
        "### Tratamento Especial aos princípios ativos\n",
        "Foi realizada uma análise exploratória preliminar no dataset candidato utilizando a ferramenta PowerBI. Esta análise se encontra em pdf no diretório deste projeto, bem como o arquivo pbix para ser aberto. Como não há certeza das versões de PowerBi pelo leitor, a análise também foi gerada em PDF.\n",
        "Observou-se que o campo descrição continha alguns desafios comuns de mineração de textos e PNL. As ações para resolver esses desafios são tomadas a partir deste ponto do notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJ6COYB8VAKV",
        "outputId": "55241c0c-3fa3-48cd-c6b3-5b17da8b183f"
      },
      "outputs": [],
      "source": [
        "# criação de uma função para transformar a lista de principios ativos na descrição em um dicionario iterável em python,\n",
        "# separando principio ativo e dosagens das quantidades e formas farmacêuticas\n",
        "def parse_description(description):\n",
        "  quantity, items = description.split(\"|\")\n",
        "  quantity = quantity.strip()\n",
        "  items_list = [item.strip() for item in items.split(\";\")]\n",
        "  items_dict = {str(i+1): item for i, item in enumerate(items_list)}\n",
        "  return quantity, items_dict\n",
        "\n",
        "# Exemplo de uso\n",
        "description = \"60 CAP | NAC  250MG; SILIMARINA  150MG; SAME  50MG\"\n",
        "quantidade, itens = parse_description(description)\n",
        "print(\"Quantidade:\", quantidade)\n",
        "print(\"Itens:\", itens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsddWqIZVAKV"
      },
      "outputs": [],
      "source": [
        "# adicionando ao Dataframe colunas separadas para principios ativos e quantidades\n",
        "df[['quantidade', 'itens']] = df['descricao'].apply(lambda x: pd.Series(parse_description(x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qW3pqcpbVAKV"
      },
      "outputs": [],
      "source": [
        "# criação de uma função para gerar um hash de identificação unitária para a receita, o que\n",
        "# será util ao longo de todo o experimento\n",
        "def generate_hash(description):\n",
        "  return hashlib.md5(description.encode()).hexdigest()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCHoedtKVAKV"
      },
      "outputs": [],
      "source": [
        "# Adicionando a coluna de hash ao dataframe\n",
        "df['hash'] = df['descricao'].apply(generate_hash)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-GENu7SVAKV"
      },
      "source": [
        "### Processamento de Linguagem Natural\n",
        "O campo descrição da receita pode conter uma informação que consideramos muito valiosa, porque hipoteticamente, o fator determinante para o preço de um medicamento são os princípios ativos que estes contém.\n",
        "Pelo motivo acima decidimos aprofundar ao máximo no tratamento do campo descrição."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRBpW1mWVAKW"
      },
      "outputs": [],
      "source": [
        "# copiando o primeiro DataFrame de forma preventiva (todas as antigas colunas, igual ao original)\n",
        "df_original = df.copy()\n",
        "\n",
        "# Criando o segundo DataFrame (apenas a coluna 'itens', expandida), mantendo o hash para não\n",
        "# perder o link com a receita que originou os itens\n",
        "item_rows = []\n",
        "for _, row in df.iterrows():\n",
        "  for key, value in row['itens'].items():\n",
        "    item_rows.append({'hash': row['hash'], 'num_item': key, 'descricao_item': value})\n",
        "\n",
        "df_itens = pd.DataFrame(item_rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xz2BD2cyVAKW"
      },
      "outputs": [],
      "source": [
        "# separação do dataframe de itens do texto do principio ativo e da dosagem, de forma que o texto possa ser\n",
        "# tratado especialmente\n",
        "df_itens[['principio_ativo', 'dosagem']] = df_itens['descricao_item'].apply(lambda x: pd.Series([ ' '.join(x.split()[:-1]), x.split()[-1] ]))\n",
        "df_itens['principio_ativo'] = df_itens['principio_ativo'].str.replace(',', '.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "T48f8sfpVAKW",
        "outputId": "207935cd-6081-46ac-e09b-6e1d4f37f2f3"
      },
      "outputs": [],
      "source": [
        "# visualizando o dataframe de itens\n",
        "display(HTML(df_itens.head(10).to_html()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2ob4QRtVAKW"
      },
      "source": [
        "#### Limpeza nos princípios ativos\n",
        "Os princípios ativos serão então isolados em um dataframe separado, onde seus nomes serão deduplicados, e assim este dataframe será\n",
        "submetido a um LLM para que se criem clusters de medicamentos. Conforme o leitor pode ver na Análise Exploratória em PowerBi, são 1632 princípios ativos descritos, que tem pequenas variações em linguagem natural que precisam ser corrigidas para um bom trabalho."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAvEIwjeVAKW"
      },
      "outputs": [],
      "source": [
        "# criação de um dataframe de princípios ativos\n",
        "df_principios_ativos = df_itens[['principio_ativo']].copy()\n",
        "\n",
        "# Agrupamento para contar quantos principios ativos iguais existem\n",
        "df_principios_ativos['quantidade'] = df_principios_ativos.groupby('principio_ativo')['principio_ativo'].transform('size')\n",
        "\n",
        "# Remover duplicatas\n",
        "df_principios_ativos = df_principios_ativos.drop_duplicates(subset='principio_ativo').reset_index(drop=True)\n",
        "# Orderar alfabeticamente\n",
        "df_principios_ativos = df_principios_ativos.sort_values(by='principio_ativo').reset_index(drop=True)\n",
        "\n",
        "# salvamento dos principios ativos para processamento do LLM\n",
        "df_principios_ativos.to_excel('./output/principios_ativos.xlsx')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8u_-tzZVAKW"
      },
      "source": [
        "#### Utilização de LLM para clusterização dos princípios ativos\n",
        "A clusterização de textos por similaridade não é uma funcionalidade nova, muito menos advinda dos LLMs, porém a título de demonstração optamos por utilizar a API da OpenAI com um prompt criado especificamente após a observaçao pela análise exploratória dos eventos mais comuns de diferença nas descrições armazenadas. O arquivo em principios_ativos.xlsx foi trabalho separadamente em um outro notebook também anexado neste trabalho. O nome deste notebook é llm_call_v2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-FfvdPlVAKW"
      },
      "outputs": [],
      "source": [
        "# recuperação do cluster de medicamentos\n",
        "cluster_medicamentos = pd.read_excel('./input/clusters.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d31OepwpVAKW",
        "outputId": "5c1a7157-154d-4526-d767-ed46990b7701"
      },
      "outputs": [],
      "source": [
        "# colunas retornadas no cluster medicamentos\n",
        "cluster_medicamentos.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XkSq3MrIVAKW"
      },
      "outputs": [],
      "source": [
        "# Adição do nome do cluster ao dataframe de itens\n",
        "df_itens = df_itens.merge(cluster_medicamentos[['original', 'cluster']], left_on='principio_ativo', right_on='original', how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3vff0yXVAKW"
      },
      "outputs": [],
      "source": [
        "# Neste ponto estamos extraindo do item a unidade e a dosagem para ter a informação separada\n",
        "df_itens['dose'] = df_itens['dosagem'].str.extract(r'(\\d+[\\.,]?\\d*)')  # Extrai a parte numérica\n",
        "df_itens['unidade'] = df_itens['dosagem'].str.extract(r'([a-zA-Z]+)')  # Extrai a parte textual (unidade de medida)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8uBQWcHVAKW"
      },
      "outputs": [],
      "source": [
        "# Salvamento do dataframe finalizado para análise exploratória no Power BI\n",
        "df_itens.to_excel('./output/itens_finalizados.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKKBfusBVAKX",
        "outputId": "98970c67-4deb-4890-f028-2a00ba77dfec"
      },
      "outputs": [],
      "source": [
        "# Avaliação das colunas do dataframe\n",
        "df_itens.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJRBY1fuVAKX"
      },
      "outputs": [],
      "source": [
        "# simplificando o dataframe a titulo de facilitar colocar mais informação para linkar com o preprocessamento de PCA abaixo\n",
        "df_itens_selecionado = df_itens[['hash', 'cluster', 'dose', 'unidade']]\n",
        "pca_preprocessing_df = df_itens_selecionado.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cu-Fg6hfVAKX",
        "outputId": "a2672991-1750-4c30-dbf9-2388fb721d18"
      },
      "outputs": [],
      "source": [
        "df_itens_selecionado.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMW5tyMbs_EF",
        "outputId": "2643d9e2-ff26-4f10-df08-849fe7c2d15d"
      },
      "outputs": [],
      "source": [
        "df_matricial = df_itens_selecionado.copy()\n",
        "\n",
        "# OneHotEncoder para o cluster (medicamentos)\n",
        "ohe = OneHotEncoder(sparse_output=False)\n",
        "clusters_encoded = ohe.fit_transform(df_matricial[['cluster']])\n",
        "cluster_columns = ohe.get_feature_names_out(['cluster'])\n",
        "\n",
        "# LabelEncoder para a unidade\n",
        "le = LabelEncoder()\n",
        "df_matricial.loc[:, 'unidade_encoded'] = le.fit_transform(df_matricial['unidade'])\n",
        "\n",
        "# Criar DataFrame com matriz one-hot de princípios ativos\n",
        "df_clusters = pd.DataFrame(clusters_encoded, columns=cluster_columns)\n",
        "df_clusters['hash'] = df_matricial['hash']\n",
        "\n",
        "# Agregar as matrizes de medicamentos, doses e unidades por hash\n",
        "df_agg = df_clusters.groupby('hash').apply(lambda x: np.vstack(x[cluster_columns].to_numpy())).reset_index(name='matrix')\n",
        "df_doses = df_matricial.groupby('hash')['dose'].apply(lambda x: np.array(x.tolist())).reset_index()\n",
        "df_unidades = df_matricial.groupby('hash')['unidade_encoded'].apply(lambda x: np.array(x.tolist())).reset_index()\n",
        "\n",
        "# Mesclar tudo no DataFrame final\n",
        "final_df = df_agg.merge(df_doses, on='hash').merge(df_unidades, on='hash')\n",
        "\n",
        "print(final_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "chb57mPdVAKX",
        "outputId": "a4cb3025-dc9f-4680-9ab8-3911c306fad4"
      },
      "outputs": [],
      "source": [
        "display(HTML(final_df.head(10).to_html()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SIG9aytVAKX",
        "outputId": "aca9c30a-2a9a-4cdd-8642-2c2b6dc52004"
      },
      "outputs": [],
      "source": [
        "final_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9hjlSAAVAKX",
        "outputId": "43f276dc-7cde-45be-9a84-77102181acb1"
      },
      "outputs": [],
      "source": [
        "print(final_df.dtypes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKFqMBooVAKX"
      },
      "source": [
        "### Transformando a quantidade de cápsulas em texto para números\n",
        "Vamos agora em passos finais, isolar a quantidade de cápsulas em uma coluna para aproveitar este valor como um dos inputs no modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A03sFJjNVAKX"
      },
      "outputs": [],
      "source": [
        "# Extrai apenas os números (inclui inteiros e decimais)\n",
        "df_original['quantidade_prescrita'] = df_original['quantidade'].str.extract(r'(\\d+\\.?\\d*)')\n",
        "\n",
        "# Converte para número (float ou int) se necessário\n",
        "df_original['quantidade_prescrita'] = pd.to_numeric(df_original['quantidade_prescrita'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yvfn4CqIVAKX",
        "outputId": "93a3e64a-5500-4917-d622-c7159e7ec815"
      },
      "outputs": [],
      "source": [
        "df_original.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "611SHJ7eVAKY"
      },
      "outputs": [],
      "source": [
        "df_treino = df_original.merge(final_df, on='hash')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lmunwAaVAKY"
      },
      "source": [
        "### Ultimos tratamentos para construi o dataset para processamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4iKMReg4VAKY"
      },
      "outputs": [],
      "source": [
        "# Transformando a data \"criado\" em unix timestamp numérico\n",
        "df_treino['criado'] = pd.to_datetime(df_treino['criado'])\n",
        "\n",
        "# Converte para timestamp em milissegundos\n",
        "df_treino['criado'] = df_treino['criado'].astype('int64') // 10**6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtOmSUTXVAKY"
      },
      "outputs": [],
      "source": [
        "# retirando do dataframe campos que não são numéricos\n",
        "df_final = df_treino.drop(['descricao', 'quantidade', 'itens'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVIKPItpVAKb",
        "outputId": "1c13a168-8e54-4155-fae4-ea6734a0da82"
      },
      "outputs": [],
      "source": [
        "# Verificando as colunas finais\n",
        "df_final.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P22l8w9QVAKb",
        "outputId": "27a2705b-0d74-4302-ef54-0f0bc6294df6"
      },
      "outputs": [],
      "source": [
        "# verificando a integridade do dataframe final com o dataframe inicial\n",
        "len(df_final) == len(df_original)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1LhwfqAVAKb",
        "outputId": "c7eb6630-b457-4b78-f6a8-135d1a81a89d"
      },
      "outputs": [],
      "source": [
        "#verificando se todos os tipos estão de acordo\n",
        "print(df_final.dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bb3bZVIyVAKb",
        "outputId": "0bd6f5cf-1b85-4b42-8e5a-3632272b8184"
      },
      "outputs": [],
      "source": [
        "display(HTML(df_final.head(10).to_html()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "id": "fNU7NjkvVAKb",
        "outputId": "95e20ed6-8bea-4cde-e8da-ffea0eb3a0dd"
      },
      "outputs": [],
      "source": [
        "df_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzvLeTvIVAKb",
        "outputId": "0b058c49-e6f8-4c0e-a882-9d3b237c8f41"
      },
      "outputs": [],
      "source": [
        "# 1 CARREGAMENTO DO DATASET\n",
        "df_tensores = df_final.copy()\n",
        "\n",
        "# 2️ TRATAMENTO DA COLUNA \"CRIADO\"\n",
        "df_tensores[\"criado\"] = pd.to_datetime(df_tensores[\"criado\"], errors=\"coerce\")  # Converter para datetime\n",
        "df_tensores[\"ano\"] = df_tensores[\"criado\"].dt.year\n",
        "df_tensores[\"mes\"] = df_tensores[\"criado\"].dt.month\n",
        "df_tensores[\"dia\"] = df_tensores[\"criado\"].dt.day\n",
        "df_tensores[\"hora\"] = df_tensores[\"criado\"].dt.hour\n",
        "\n",
        "# 3 PRÉ-PROCESSAMENTO PADRÃO (Matrix, Padding, Normalização)\n",
        "df_tensores[\"matrix\"] = df_tensores[\"matrix\"].apply(lambda x: np.array(eval(x)) if isinstance(x, str) else np.array(x))\n",
        "df_tensores[\"dose\"] = df_tensores[\"dose\"].apply(lambda x: eval(x) if isinstance(x, str) else x)\n",
        "df_tensores[\"unidade_encoded\"] = df_tensores[\"unidade_encoded\"].apply(lambda x: eval(x) if isinstance(x, str) else x)\n",
        "\n",
        "max_qtdInsumos = df_tensores[\"qtdInsumos\"].max()\n",
        "\n",
        "def pad_matrix(matrix, qtdInsumos, max_qtdInsumos):\n",
        "  matrix = np.array(matrix)[:qtdInsumos]\n",
        "  pad_size = max_qtdInsumos - matrix.shape[0]\n",
        "  if pad_size > 0:\n",
        "    padding = np.zeros((pad_size, 940))\n",
        "    matrix = np.vstack([matrix, padding])\n",
        "  return matrix\n",
        "\n",
        "df_tensores[\"matrix\"] = df_tensores.apply(lambda row: pad_matrix(row[\"matrix\"], row[\"qtdInsumos\"], max_qtdInsumos), axis=1)\n",
        "\n",
        "df_tensores[\"dose\"] = df_tensores.apply(lambda row: row[\"dose\"][:row[\"qtdInsumos\"]], axis=1)\n",
        "df_tensores[\"unidade_encoded\"] = df_tensores.apply(lambda row: row[\"unidade_encoded\"][:row[\"qtdInsumos\"]], axis=1)\n",
        "\n",
        "df_tensores[\"dose\"] = pad_sequences(df_tensores[\"dose\"], maxlen=max_qtdInsumos, dtype=\"float32\", padding=\"post\").tolist()\n",
        "df_tensores[\"unidade_encoded\"] = pad_sequences(df_tensores[\"unidade_encoded\"], maxlen=max_qtdInsumos, dtype=\"int32\", padding=\"post\").tolist()\n",
        "\n",
        "# Normalização e salvamento do Scaler\n",
        "scaler = MinMaxScaler()\n",
        "df_tensores[[\"calculado\", \"quantidade_prescrita\"]] = scaler.fit_transform(df_tensores[[\"calculado\", \"quantidade_prescrita\"]])\n",
        "\n",
        "joblib.dump(scaler, \"./output/scaler.pkl\") # Salvando o scaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkUuGY4GVAKc",
        "outputId": "2d36c9e5-101b-4733-c3ac-0369a9dc1767"
      },
      "outputs": [],
      "source": [
        "df_determinante = df_tensores.copy()\n",
        "df_determinante[\"matrix_real\"] = df_determinante[\"matrix\"].apply(lambda x: mmh3.hash(str(x.flatten().tolist()), signed=False) / (2**32))\n",
        "df_determinante[\"dose_real\"] = df_determinante[\"dose\"].apply(lambda x: mmh3.hash(str(x), signed=False) / (2**32))\n",
        "df_determinante[\"unidade_encoded_real\"] = df_determinante[\"unidade_encoded\"].apply(lambda x: mmh3.hash(str(x), signed=False) / (2**32))\n",
        "df_determinante[\"criado\"] = pd.to_datetime(df_determinante['criado']).astype('int64') // 10**6\n",
        "df_determinante = df_determinante.drop(columns=['matrix', 'dose', 'unidade_encoded','ano','mes', 'dia', 'hora'])\n",
        "df_determinante.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_tensores.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_determinante.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pca_preprocessing_df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llMRQdLgVAKc"
      },
      "source": [
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOHJPVC5VAKc"
      },
      "source": [
        "# PCA\n",
        "notebook 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_L2DlbnVAKc"
      },
      "source": [
        "## Enconding\n",
        "Por se tratar de um caso de regressão, precisamos encontrar uma forma harmônica de representar os principios ativos e suas dosagens de maneira a conseguir transformar os mesmos, que são categóricos, em valores numéricos.\n",
        "A execução da clusterização já foi feita com o objetivo de obviamente deduplicar, mas também simplificar e reduzir o número de categorias para realizar o encoding. Como pode ser visto na Análise Exploratória, reduzimos os princípios ativos de 1632 descrições para 940 clusters.<br>\n",
        "Vários encoders são candidatos para serem usados neste momento. A título de tempo disponível utilizaremos uma abordagem onde a dose e a unidade serão embutidas no encoder.<br>\n",
        "ATENÇÃO: o processo de encoder precisa ser pensado com muitíssimo cuidado. Neste experimento simplesmente implementamos a abordagem mais rápida possível devido ao tempo disponível para implementar e entregar. Muitas melhorias podem ser feitas neste processo, e seria de grande valor uma conversa 1:1 com o leitor para explicar melhor os prós, contras, riscos e mitigações deste enconder executado.\n",
        "Enfim, neste caso o bom seria inimigo do ótimo, então seguimos em frente com foco na entrega de um MVP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwGDohtoVAKc",
        "outputId": "c3c2770f-dca8-4e30-b491-a85e6b76c372"
      },
      "outputs": [],
      "source": [
        "# geracão de uma sigla com o nome do principio ativo tratando duplicados para simplificar o nome da coluna do encoder\n",
        "siglas = {}\n",
        "def gerar_sigla(cluster):\n",
        "    partes = cluster.split()\n",
        "    sigla = ''.join([p[:3].upper() for p in partes])  # Usa 3 primeiras letras de cada palavra\n",
        "    original_sigla = sigla\n",
        "    contador = 1\n",
        "    while sigla in siglas:  # Se já existe, adiciona um número incremental\n",
        "        sigla = original_sigla + str(contador)\n",
        "        contador += 1\n",
        "    siglas[sigla] = cluster\n",
        "    return sigla\n",
        "#  criação da sigla no dataframe\n",
        "\n",
        "pca_preprocessing_df['cluster_sigla'] = pca_preprocessing_df['cluster'].apply(gerar_sigla)\n",
        "\n",
        "# criação de um campo que será chave no encoder com a sigla+dose+unidade\n",
        "pca_preprocessing_df['key'] = pca_preprocessing_df['cluster_sigla'] + '_' + pca_preprocessing_df['dose'].astype(str) + '_' + pca_preprocessing_df['unidade']\n",
        "# Aplicar OneHotEncoder na chave reduzida\n",
        "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
        "encoded_values = one_hot_encoder.fit_transform(pca_preprocessing_df[['key']])\n",
        "\n",
        "# Criar um DataFrame com as variáveis codificadas e nomes reduzidos\n",
        "encoded_columns = one_hot_encoder.get_feature_names_out(['key'])\n",
        "encoded_columns = [col.replace('key_', '') for col in encoded_columns]  # Remover prefixo desnecessário\n",
        "\n",
        "encoded_df = pd.DataFrame(encoded_values, columns=encoded_columns)\n",
        "\n",
        "# Concatenar os dados codificados ao DataFrame original\n",
        "pca_preprocessing_encoded_df = pd.concat([pca_preprocessing_df[['hash']], encoded_df], axis=1)\n",
        "\n",
        "# Agregar por 'hash' e usar 'max' para manter a presença dos valores\n",
        "pca_preprocessing_aggregated_df = pca_preprocessing_encoded_df.groupby('hash').agg('max').reset_index()\n",
        "\n",
        "# Exibir o resultado final\n",
        "print(pca_preprocessing_aggregated_df)# Exibir o resultado final\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "q77pGgxDVAKc",
        "outputId": "122204dd-8e72-4093-8b5e-708ae455db02"
      },
      "outputs": [],
      "source": [
        "display(HTML(pca_preprocessing_aggregated_df.head(10).to_html()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyAbe68vVAKc"
      },
      "source": [
        "#### Redução de Dimensionalidade\n",
        "Conforme pode ser visto acima, a combinatória de principio ativo (em cluster), dosagem e unidade gerou um encoder com 23355 colunas. Como prática neste campo da análise de regressão, precisamos executar uma redução de dimensionalidade para termos um dataset de treinamento, teste e validação viável para executar o experimento.\n",
        "Tentamos então duas técnicas para escolher o melhor resultado:\n",
        "- Análise de Componentes Principais (PCA)\n",
        "- Truncated Singular Value Decomposition (TruncatedSVD)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOuGbQ4TVAKc"
      },
      "source": [
        "<hr>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWFDgXNfVAKc"
      },
      "outputs": [],
      "source": [
        "X = pca_preprocessing_aggregated_df.drop(columns=['hash'])  # Remover a coluna 'hash' antes da redução\n",
        "\n",
        "# Normalizar os dados antes do PCA\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Reduzindo para 50 componentes principais com Truncated SVD\n",
        "svd = TruncatedSVD(n_components=50, random_state=42)\n",
        "X_svd = svd.fit_transform(X_scaled)\n",
        "\n",
        "# Testando PCA para comparação\n",
        "pca = PCA(n_components=50, random_state=42)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Criar DataFrames com os componentes reduzidos\n",
        "df_svd = pd.DataFrame(X_svd, columns=[f'comp_{i+1}' for i in range(50)])\n",
        "df_pca = pd.DataFrame(X_pca, columns=[f'pca_{i+1}' for i in range(50)])\n",
        "\n",
        "# Adicionar a coluna 'hash' de volta para identificação\n",
        "df_svd.insert(0, 'hash', pca_preprocessing_aggregated_df['hash'])\n",
        "df_pca.insert(0, 'hash', pca_preprocessing_aggregated_df['hash'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udTe5yzWVAKd"
      },
      "outputs": [],
      "source": [
        "# Comparar a variância explicada\n",
        "var_svd = np.sum(svd.explained_variance_ratio_)\n",
        "var_pca = np.sum(pca.explained_variance_ratio_)\n",
        "\n",
        "print(f\"Variância explicada pelos 50 componentes:\")\n",
        "print(f\"Truncated SVD: {var_svd:.2%}\")\n",
        "print(f\"PCA: {var_pca:.2%}\")\n",
        "\n",
        "# Plotando os componentes principais dos dois métodos\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "sns.scatterplot(x=df_svd['comp_1'], y=df_svd['comp_2'], alpha=0.5, ax=axes[0])\n",
        "axes[0].set_title(\"Truncated SVD (Comp 1 vs Comp 2)\")\n",
        "\n",
        "sns.scatterplot(x=df_pca['pca_1'], y=df_pca['pca_2'], alpha=0.5, ax=axes[1])\n",
        "axes[1].set_title(\"PCA (Comp 1 vs Comp 2)\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhIy2ALjVAKd"
      },
      "source": [
        "##### Vamos prosseguir o trabalho utilizando o PCA, simplesmente pela variancia semelhante e preferência pessoal pela técnica"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFyh97l5VAKd"
      },
      "source": [
        "### Transformando a quantidade de cápsulas em texto para números\n",
        "Vamos agora em passos finais, isolar a quantidade de cápsulas em uma coluna para aproveitar este valor como um dos inputs no modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6QSC1WHVAKd"
      },
      "outputs": [],
      "source": [
        "# Extrai apenas os números (inclui inteiros e decimais)\n",
        "df_original['quantidade_prescrita'] = df_original['quantidade'].str.extract(r'(\\d+\\.?\\d*)')\n",
        "\n",
        "# Converte para número (float ou int) se necessário\n",
        "df_original['quantidade_prescrita'] = pd.to_numeric(df_original['quantidade_prescrita'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upkNarhLVAKd"
      },
      "outputs": [],
      "source": [
        "df_original.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8n_Ye5uPVAKd"
      },
      "source": [
        "### Ultimos tratamentos para construi o dataset para processamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sD_GWXF3VAKd"
      },
      "outputs": [],
      "source": [
        "# Juntando o dataframe original com o dataframe representativo em PCA dos principios ativos\n",
        "pca_preprocessed_df = pd.merge(df_original, df_pca, on='hash', how='inner')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXTKMc9PVAKd"
      },
      "outputs": [],
      "source": [
        "# Transformando a data \"criado\" em unix timestamp numérico\n",
        "pca_preprocessed_df['criado'] = pd.to_datetime(pca_preprocessed_df['criado'])\n",
        "\n",
        "# Converte para timestamp em milissegundos\n",
        "pca_preprocessed_df['criado'] = pca_preprocessed_df['criado'].astype('int64') // 10**6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3h6ifHZVAKd"
      },
      "outputs": [],
      "source": [
        "# retirando do dataframe campos que não são numéricos\n",
        "pca_preprocessed_df = pca_preprocessed_df.drop(['descricao', 'quantidade', 'itens'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HD22yvIEVAKd"
      },
      "outputs": [],
      "source": [
        "# Verificando as colunas finais\n",
        "pca_preprocessed_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "drQQit5jVAKd"
      },
      "outputs": [],
      "source": [
        "# Verificando a integridade do dataframe final com o dataframe inicial\n",
        "len(pca_preprocessed_df) == len(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhbNXyt9VAKd"
      },
      "outputs": [],
      "source": [
        "# verificando se todos os tipos estão de acordo\n",
        "print(pca_preprocessed_df.dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jBSjdhS2VAKe"
      },
      "outputs": [],
      "source": [
        "pca_preprocessed_df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Treinamento\n",
        "Posto que atingimos um dataset com número de parâmetros de acordo para o treinamento de um modelo de regressão, vamos seguir o experimento proponto três modelos distintos para escolher, em teste, qual deles tem a melhor performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Train Test Split\n",
        "Com retirada antecipada de 2000 elementos aleatórios para validação posterior"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Passo 1: Separar as 2000 linhas para validação conforme orientado \n",
        "pca_receitas_validation_df = pca_preprocessed_df.sample(n=2000, random_state=42)\n",
        "\n",
        "# O restante do dataset será usado para treinamento e teste\n",
        "pca_receitas_train_test_df = pca_preprocessed_df.drop(pca_receitas_validation_df.index)\n",
        "\n",
        "# Passo 2: Divisão entre treino e teste (80% treino, 20% teste)\n",
        "X = pca_receitas_train_test_df.drop(columns=['correto', 'hash'])\n",
        "y = pca_receitas_train_test_df['correto']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Normalização"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Passo 3: Pré-processamento - Normalização dos dados numéricos\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Normalizando os dados de treino e teste\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Árvore de Descisão"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Modelo 1: Árvore de Decisão\n",
        "tree_model = DecisionTreeRegressor(random_state=42)\n",
        "tree_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Avaliação do modelo - Árvore de Decisão\n",
        "y_pred_tree = tree_model.predict(X_test_scaled)\n",
        "rmse_tree = np.sqrt(mean_squared_error(y_test, y_pred_tree))\n",
        "mae_tree = mean_absolute_error(y_test, y_pred_tree)\n",
        "print(f\"Árvore de Decisão - RMSE: {rmse_tree:.4f}, MAE: {mae_tree:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Modelo 2: Random Forest Regressor\n",
        "rf_model = RandomForestRegressor(random_state=42)\n",
        "rf_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Avaliação do modelo - Random Forest\n",
        "y_pred_rf = rf_model.predict(X_test_scaled)\n",
        "rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
        "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
        "print(f\"Random Forest - RMSE: {rmse_rf:.4f}, MAE: {mae_rf:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Modelo 3: XGBoost Regressor\n",
        "xgb_model = xgb.XGBRegressor(random_state=42)\n",
        "xgb_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Avaliação do modelo - XGBoost\n",
        "y_pred_xgb = xgb_model.predict(X_test_scaled)\n",
        "rmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb))\n",
        "mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
        "print(f\"XGBoost - RMSE: {rmse_xgb:.4f}, MAE: {mae_xgb:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Passo 8: Explicabilidade com SHAP (XGBoost)\n",
        "explainer = shap.Explainer(xgb_model, X_train_scaled)\n",
        "shap_values = explainer(X_test_scaled)\n",
        "\n",
        "# Visualizando o gráfico de importância das variáveis\n",
        "shap.summary_plot(shap_values, X_test)\n",
        "\n",
        "# Passo 9: Usando o modelo XGBoost para prever no conjunto de validação\n",
        "X_validation = pca_receitas_validation_df.drop(columns=['correto', 'hash'])\n",
        "X_validation_scaled = scaler.transform(X_validation)\n",
        "\n",
        "y_validation_pred = xgb_model.predict(X_validation_scaled)\n",
        "\n",
        "# Visualizando as predições para o conjunto de validação\n",
        "pca_receitas_validation_df['predito'] = y_validation_pred\n",
        "\n",
        "# Se desejar salvar as predições para análises futuras\n",
        "pca_receitas_validation_df[['hash', 'correto', 'predito']].to_csv('./output/predicoes_validacao_pca.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2FWk7ypVAKe"
      },
      "source": [
        "# Tensores\n",
        "notebook 4 (rede neural)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTNxIS9GVAKe"
      },
      "outputs": [],
      "source": [
        "# SEPARAÇÃO DOS DADOS\n",
        "df_tensores_validation = df_tensores.sample(n=2000, random_state=42)\n",
        "df_tensores_train_test = df_tensores.drop(df_tensores_validation.index)\n",
        "validation_hashes = df_tensores_validation[\"hash\"].values\n",
        "\n",
        "X_matrix = np.stack(df_tensores_train_test[\"matrix\"].values)\n",
        "X_dose = np.stack(df_tensores_train_test[\"dose\"].values)\n",
        "X_unidade = np.stack(df_tensores_train_test[\"unidade_encoded\"].values)\n",
        "\n",
        "X_numeric = df_tensores_train_test[[\"qtdInsumos\", \"calculado\", \"quantidade_prescrita\", \"ano\", \"mes\", \"dia\", \"hora\"]].values\n",
        "\n",
        "y = df_tensores_train_test[\"correto\"].values\n",
        "\n",
        "X = np.hstack([\n",
        "  X_numeric,\n",
        "  X_matrix.reshape(len(X_matrix), -1),\n",
        "  X_dose,\n",
        "  X_unidade\n",
        "])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcVnb8T2VAKe"
      },
      "outputs": [],
      "source": [
        "df_tensores_train_test.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 1º - MAE: 79.2750 RMSE: 272.077\n",
        "\n",
        "# # 5️⃣ CONSTR1els.Sequential([\n",
        "#     layers.Dense(256, activation=\"relu\", input_shape=(X_train.shape[1],)),\n",
        "#     layers.Dropout(0.3),\n",
        "#     layers.Dense(128, activation=\"relu\"),\n",
        "#     layers.Dropout(0.2),\n",
        "#     layers.Dense(64, activation=\"relu\"),\n",
        "#     layers.Dense(1)\n",
        "# ])\n",
        "\n",
        "# model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
        "\n",
        "# # 6️⃣ TREINAMENTO\n",
        "# model.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(X_test, y_test))\n",
        "\n",
        "# # 7️⃣ SALVAR O MODELO\n",
        "# model.save(\"./output/models/modelo_tensorflow.keras\")  # Salvar modelo no formato TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 2º - MAE: 81.1083 RMSE: 274.1747\n",
        "\n",
        "# from tensorflow.keras import models, layers, regularizers, callbacks\n",
        "\n",
        "# model = models.Sequential([\n",
        "#   layers.Dense(512, activation=\"relu\", input_shape=(X_train.shape[1],),\n",
        "#     kernel_regularizer=regularizers.l2(0.001)),\n",
        "#   layers.BatchNormalization(),\n",
        "#   layers.Dropout(0.3),\n",
        "  \n",
        "#   layers.Dense(256, activation=\"relu\", kernel_regularizer=regularizers.l2(0.001)),\n",
        "#   layers.BatchNormalization(),\n",
        "#   layers.Dropout(0.3),\n",
        "  \n",
        "#   layers.Dense(128, activation=\"relu\", kernel_regularizer=regularizers.l2(0.001)),\n",
        "#   layers.BatchNormalization(),\n",
        "#   layers.Dropout(0.2),\n",
        "  \n",
        "#   layers.Dense(64, activation=\"relu\"),\n",
        "#   layers.BatchNormalization(),\n",
        "#   layers.Dropout(0.1),\n",
        "  \n",
        "#   layers.Dense(1)\n",
        "# ])\n",
        "\n",
        "# model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
        "\n",
        "# # Definindo callbacks para melhorar o treinamento\n",
        "# early_stop = callbacks.EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
        "# reduce_lr = callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=5, min_lr=1e-6)\n",
        "\n",
        "# model.fit(\n",
        "#   X_train, y_train,\n",
        "#   epochs=100,  # aumentando o número de épocas para dar mais tempo de aprendizado\n",
        "#   batch_size=64,\n",
        "#   validation_data=(X_test, y_test),\n",
        "#   callbacks=[early_stop, reduce_lr]\n",
        "# )\n",
        "\n",
        "# model.save(\"./output/models/modelo_tensorflow_refinado.keras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3º - MAE: 68.8011  RMSE: 255.5577 Rede neural multilayer perceptron\n",
        "\n",
        "\n",
        "model = models.Sequential([\n",
        "  layers.Dense(1024, activation=\"relu\", input_shape=(X_train.shape[1],),\n",
        "    kernel_regularizer=regularizers.l2(0.001)),\n",
        "  layers.BatchNormalization(),\n",
        "  layers.Dropout(0.3),\n",
        "\n",
        "  layers.Dense(512, activation=\"relu\", kernel_regularizer=regularizers.l2(0.001)),\n",
        "  layers.BatchNormalization(),\n",
        "  layers.Dropout(0.3),\n",
        "\n",
        "  layers.Dense(256, activation=\"relu\", kernel_regularizer=regularizers.l2(0.001)),\n",
        "  layers.BatchNormalization(),\n",
        "  layers.Dropout(0.3),\n",
        "\n",
        "  layers.Dense(128, activation=\"relu\", kernel_regularizer=regularizers.l2(0.001)),\n",
        "  layers.BatchNormalization(),\n",
        "  layers.Dropout(0.2),\n",
        "\n",
        "  layers.Dense(64, activation=\"relu\", kernel_regularizer=regularizers.l2(0.001)),\n",
        "  layers.BatchNormalization(),\n",
        "  layers.Dropout(0.2),\n",
        "\n",
        "  layers.Dense(32, activation=\"relu\", kernel_regularizer=regularizers.l2(0.001)),\n",
        "  layers.BatchNormalization(),\n",
        "  layers.Dropout(0.1),\n",
        "\n",
        "  layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
        "\n",
        "early_stop = callbacks.EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
        "reduce_lr = callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=5, min_lr=1e-6)\n",
        "\n",
        "model.fit(\n",
        "  X_train, y_train,\n",
        "  epochs=200,\n",
        "  batch_size=64,\n",
        "  validation_data=(X_test, y_test),\n",
        "  callbacks=[early_stop, reduce_lr]\n",
        ")\n",
        "\n",
        "model.save(\"./output/models/modelo_tensorflow_profundo.keras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 4º - MAE: 82.1332  RMSE: 263.7608\n",
        "\n",
        "# from tensorflow.keras import models, layers, regularizers, callbacks\n",
        "# from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, add, LeakyReLU\n",
        "\n",
        "# # Entrada\n",
        "# input_layer = Input(shape=(X_train.shape[1],))\n",
        "\n",
        "# # Primeira camada densa\n",
        "# x = Dense(1024, kernel_regularizer=regularizers.l2(0.001))(input_layer)\n",
        "# x = BatchNormalization()(x)\n",
        "# x = LeakyReLU(alpha=0.1)(x)\n",
        "# x = Dropout(0.3)(x)\n",
        "\n",
        "# # Bloco Residual 1\n",
        "# res = Dense(512, kernel_regularizer=regularizers.l2(0.001))(x)\n",
        "# res = BatchNormalization()(res)\n",
        "# res = LeakyReLU(alpha=0.1)(res)\n",
        "# res = Dropout(0.3)(res)\n",
        "\n",
        "# # Atalho para ajustar a dimensão\n",
        "# shortcut = Dense(512, kernel_regularizer=regularizers.l2(0.001))(x)\n",
        "# shortcut = BatchNormalization()(shortcut)\n",
        "\n",
        "# # Soma do bloco residual com o atalho\n",
        "# x = add([res, shortcut])\n",
        "# x = LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "# # Bloco Residual 2\n",
        "# res = Dense(256, kernel_regularizer=regularizers.l2(0.001))(x)\n",
        "# res = BatchNormalization()(res)\n",
        "# res = LeakyReLU(alpha=0.1)(res)\n",
        "# res = Dropout(0.3)(res)\n",
        "\n",
        "# # Atalho para o segundo bloco\n",
        "# shortcut = Dense(256, kernel_regularizer=regularizers.l2(0.001))(x)\n",
        "# shortcut = BatchNormalization()(shortcut)\n",
        "\n",
        "# x = add([res, shortcut])\n",
        "# x = LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "# # Camadas subsequentes sem conexão residual\n",
        "# x = Dense(128, activation=\"relu\", kernel_regularizer=regularizers.l2(0.001))(x)\n",
        "# x = BatchNormalization()(x)\n",
        "# x = Dropout(0.2)(x)\n",
        "\n",
        "# x = Dense(64, activation=\"relu\", kernel_regularizer=regularizers.l2(0.001))(x)\n",
        "# x = BatchNormalization()(x)\n",
        "# x = Dropout(0.2)(x)\n",
        "\n",
        "# x = Dense(32, activation=\"relu\", kernel_regularizer=regularizers.l2(0.001))(x)\n",
        "# x = BatchNormalization()(x)\n",
        "# x = Dropout(0.1)(x)\n",
        "\n",
        "# # Camada de saída com ativação linear para regressão\n",
        "# output = Dense(1, activation=\"linear\")(x)\n",
        "\n",
        "# model = models.Model(inputs=input_layer, outputs=output)\n",
        "\n",
        "# model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
        "\n",
        "# # Callbacks para monitoramento\n",
        "# early_stop = callbacks.EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
        "# reduce_lr = callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=5, min_lr=1e-6)\n",
        "\n",
        "# model.fit(\n",
        "#   X_train, y_train,\n",
        "#   epochs=200,\n",
        "#   batch_size=64,\n",
        "#   validation_data=(X_test, y_test),\n",
        "#   callbacks=[early_stop, reduce_lr]\n",
        "# )\n",
        "\n",
        "# model.save(\"./output/models/modelo_tensorflow_residual.keras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # 5º - MAE: 77.7278  RMSE: 274.1434\n",
        "\n",
        "# from tensorflow.keras import models, layers, regularizers, callbacks\n",
        "# from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, add, LeakyReLU\n",
        "\n",
        "# # Entrada\n",
        "# input_layer = Input(shape=(X_train.shape[1],))\n",
        "\n",
        "# x = Dense(1024, kernel_regularizer=regularizers.l2(0.0005))(input_layer)\n",
        "# x = BatchNormalization()(x)\n",
        "# x = LeakyReLU(alpha=0.01)(x)\n",
        "# x = Dropout(0.2)(x)\n",
        "\n",
        "# # Primeiro bloco residual\n",
        "# res = Dense(512, kernel_regularizer=regularizers.l2(0.0005))(x)\n",
        "# res = BatchNormalization()(res)\n",
        "# res = LeakyReLU(alpha=0.01)(res)\n",
        "# res = Dropout(0.2)(res)\n",
        "\n",
        "# shortcut = Dense(512, kernel_regularizer=regularizers.l2(0.0005))(x)\n",
        "# shortcut = BatchNormalization()(shortcut)\n",
        "\n",
        "# x = add([res, shortcut])\n",
        "# x = LeakyReLU(alpha=0.01)(x)\n",
        "\n",
        "# # Segundo bloco residual\n",
        "# res = Dense(256, kernel_regularizer=regularizers.l2(0.0005))(x)\n",
        "# res = BatchNormalization()(res)\n",
        "# res = LeakyReLU(alpha=0.01)(res)\n",
        "# res = Dropout(0.2)(res)\n",
        "\n",
        "# shortcut = Dense(256, kernel_regularizer=regularizers.l2(0.0005))(x)\n",
        "# shortcut = BatchNormalization()(shortcut)\n",
        "\n",
        "# x = add([res, shortcut])\n",
        "# x = LeakyReLU(alpha=0.01)(x)\n",
        "\n",
        "# # Camadas adicionais sem conexão residual\n",
        "# x = Dense(128, kernel_regularizer=regularizers.l2(0.0005))(x)\n",
        "# x = BatchNormalization()(x)\n",
        "# x = LeakyReLU(alpha=0.01)(x)\n",
        "# x = Dropout(0.15)(x)\n",
        "\n",
        "# x = Dense(64, kernel_regularizer=regularizers.l2(0.0005))(x)\n",
        "# x = BatchNormalization()(x)\n",
        "# x = LeakyReLU(alpha=0.01)(x)\n",
        "# x = Dropout(0.15)(x)\n",
        "\n",
        "# x = Dense(32, kernel_regularizer=regularizers.l2(0.0005))(x)\n",
        "# x = BatchNormalization()(x)\n",
        "# x = LeakyReLU(alpha=0.01)(x)\n",
        "# x = Dropout(0.1)(x)\n",
        "\n",
        "# # Camada de saída (regressão)\n",
        "# output = Dense(1, activation=\"linear\")(x)\n",
        "\n",
        "# model = models.Model(inputs=input_layer, outputs=output)\n",
        "\n",
        "# model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
        "\n",
        "# early_stop = callbacks.EarlyStopping(monitor=\"val_loss\", patience=15, restore_best_weights=True)\n",
        "# reduce_lr = callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=5, min_lr=1e-7)\n",
        "\n",
        "# model.fit(\n",
        "#   X_train, y_train,\n",
        "#   epochs=300,\n",
        "#   batch_size=64,\n",
        "#   validation_data=(X_test, y_test),\n",
        "#   callbacks=[early_stop, reduce_lr]\n",
        "# )\n",
        "\n",
        "# model.save(\"./output/models/modelo_tensorflow_residual_ajustado.keras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# 8️ AVALIAÇÃO\n",
        "def evaluate_model(model, X, y, dataset_name):\n",
        "  y_pred = model.predict(X)\n",
        "  mae = mean_absolute_error(y, y_pred)\n",
        "  rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
        "  print(f\"\\n Avaliação no conjunto {dataset_name}:\")\n",
        "  print(f\"    MAE:  {mae:.4f}\")\n",
        "  print(f\"    RMSE: {rmse:.4f}\")\n",
        "  return mae, rmse\n",
        "\n",
        "# Avaliação no conjunto de teste\n",
        "evaluate_model(model, X_test, y_test, \"TESTE\")\n",
        "\n",
        "# Avaliação no conjunto de validação\n",
        "X_validation_matrix = np.stack(df_tensores_validation[\"matrix\"].values)\n",
        "X_validation_dose = np.stack(df_tensores_validation[\"dose\"].values)\n",
        "X_validation_unidade = np.stack(df_tensores_validation[\"unidade_encoded\"].values)\n",
        "\n",
        "X_validation_numeric = df_tensores_validation[[\"qtdInsumos\", \"calculado\", \"quantidade_prescrita\", \"ano\", \"mes\", \"dia\", \"hora\"]].values\n",
        "\n",
        "X_validation = np.hstack([\n",
        "  X_validation_numeric,  \n",
        "  X_validation_matrix.reshape(len(X_validation_matrix), -1),  \n",
        "  X_validation_dose,  \n",
        "  X_validation_unidade\n",
        "])\n",
        "\n",
        "evaluate_model(model, X_validation, df_tensores_validation[\"correto\"].values, \"VALIDAÇÃO\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Geração dos valores preditos para o conjunto de validação\n",
        "y_validation_pred = model.predict(X_validation).flatten()\n",
        "\n",
        "# Criação do DataFrame com valores reais e preditos\n",
        "df_val_resultado = df_tensores_validation.copy()\n",
        "df_val_resultado[\"predito\"] = y_validation_pred\n",
        "\n",
        "# Se quiser salvar em CSV\n",
        "df_val_resultado[[\"hash\", \"criado\", \"qtdInsumos\", \"calculado\", \"correto\", \"quantidade_prescrita\", \"predito\"]].to_csv(\"./output/precicoes_validacao_tensores.csv\", index=False, sep=\"|\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Resultado da otimização\n",
        "Não cremos que a otimização possa trazer resultados muito diferentes, então finalizamos o experimento.\n",
        "O arquivo Power BI de análise exploratória descreve o comportamento dos dados de validação."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ig-_UmjbVAKe"
      },
      "source": [
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mrfcydz1VAKe"
      },
      "source": [
        "# Determinantes\n",
        "notebook 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_nATbGuVAKe"
      },
      "source": [
        "#### Train Test Split\n",
        "Com retirada antecipada de 2000 elementos aleatórios para validação posterior"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1g8rQQYPVAKe"
      },
      "outputs": [],
      "source": [
        "# Passo 1: Separar as 2000 linhas para validação conforme orientado\n",
        "determinante_preprocessed_df = df_determinante.copy()\n",
        "df_determinante_receitas_validation = determinante_preprocessed_df.sample(n=2000, random_state=42)\n",
        "\n",
        "# O restante do dataset será usado para treinamento e teste\n",
        "df_determinante_receitas_train_test = determinante_preprocessed_df.drop(df_determinante_receitas_validation.index)\n",
        "\n",
        "# Passo 2: Divisão entre treino e teste (80% treino, 20% teste)\n",
        "X = df_determinante_receitas_train_test.drop(columns=['correto', 'hash'])\n",
        "y = df_determinante_receitas_train_test['correto']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4ViKQYsVAKe"
      },
      "outputs": [],
      "source": [
        "X.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suEEBGbcVAKe"
      },
      "source": [
        "#### Normalização"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOf-aMJUVAKf"
      },
      "outputs": [],
      "source": [
        "# Passo 3: Pré-processamento - Normalização dos dados numéricos\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Normalizando os dados de treino e teste\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Árvore de Descisão"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Modelo 1: Árvore de Decisão\n",
        "tree_model = DecisionTreeRegressor(random_state=42)\n",
        "tree_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Avaliação do modelo - Árvore de Decisão\n",
        "y_pred_tree = tree_model.predict(X_test_scaled)\n",
        "rmse_tree = np.sqrt(mean_squared_error(y_test, y_pred_tree))\n",
        "mae_tree = mean_absolute_error(y_test, y_pred_tree)\n",
        "print(f\"Árvore de Decisão - RMSE: {rmse_tree:.4f}, MAE: {mae_tree:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Modelo 2: Random Forest Regressor\n",
        "rf_model = RandomForestRegressor(random_state=42)\n",
        "rf_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Avaliação do modelo - Random Forest\n",
        "y_pred_rf = rf_model.predict(X_test_scaled)\n",
        "rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
        "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
        "print(f\"Random Forest - RMSE: {rmse_rf:.4f}, MAE: {mae_rf:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Modelo 3: XGBoost Regressor\n",
        "xgb_model = xgb.XGBRegressor(random_state=42)\n",
        "xgb_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Avaliação do modelo - XGBoost\n",
        "y_pred_xgb = xgb_model.predict(X_test_scaled)\n",
        "rmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb))\n",
        "mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
        "print(f\"XGBoost - RMSE: {rmse_xgb:.4f}, MAE: {mae_xgb:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Passo 8: Explicabilidade com SHAP (XGBoost)\n",
        "explainer = shap.Explainer(xgb_model, X_train_scaled)\n",
        "shap_values = explainer(X_test_scaled)\n",
        "\n",
        "# Visualizando o gráfico de importância das variáveis\n",
        "shap.summary_plot(shap_values, X_test)\n",
        "\n",
        "# Passo 9: Usando o modelo XGBoost para prever no conjunto de validação\n",
        "X_validation = df_determinante_receitas_validation.drop(columns=['correto', 'hash'])\n",
        "X_validation_scaled = scaler.transform(X_validation)\n",
        "\n",
        "y_validation_pred = xgb_model.predict(X_validation_scaled)\n",
        "\n",
        "# Visualizando as predições para o conjunto de validação\n",
        "df_determinante_receitas_validation['predito'] = y_validation_pred\n",
        "\n",
        "# Se desejar salvar as predições para análises futuras\n",
        "df_determinante_receitas_validation[['hash', 'correto', 'predito']].to_csv('./output/predicoes_validacao_determinante.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vL26JauVAKf"
      },
      "source": [
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkY9I_YwVAKf"
      },
      "source": [
        "# Simplificado (sem calculado)\n",
        "notebook 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ue0dpeXzVAKf",
        "vscode": {
          "languageId": "bat"
        }
      },
      "source": [
        "#### Train Test Split\n",
        "Com retirada antecipada de 2000 elementos aleatórios para validação posterior"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3qfvbdGVAKf"
      },
      "outputs": [],
      "source": [
        "# Passo 1: Removendo coluna calculado\n",
        "simplificado_preprocessing_df = df_determinante.drop(columns=['calculado'])\n",
        "\n",
        "# Passo 1: Separar as 2000 linhas para validação conforme orientado\n",
        "df_simplificado_receitas_validation = simplificado_preprocessing_df.sample(n=2000, random_state=42)\n",
        "\n",
        "# O restante do dataset será usado para treinamento e teste\n",
        "df_simplificado_receitas_train_test = simplificado_preprocessing_df.drop(df_simplificado_receitas_validation.index)\n",
        "\n",
        "# Passo 2: Divisão entre treino e teste (80% treino, 20% teste)\n",
        "X = df_simplificado_receitas_train_test.drop(columns=['correto', 'hash'])\n",
        "y = df_simplificado_receitas_train_test['correto']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnGe96JZVAKf"
      },
      "outputs": [],
      "source": [
        "X.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TeCEYTBVAKf"
      },
      "source": [
        "#### Normalização"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N38o-L0zVAKf"
      },
      "outputs": [],
      "source": [
        "# Passo 3: Pré-processamento - Normalização dos dados numéricos\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Normalizando os dados de treino e teste\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Árvore de Descisão"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Modelo 1: Árvore de Decisão\n",
        "tree_model = DecisionTreeRegressor(random_state=42)\n",
        "tree_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Avaliação do modelo - Árvore de Decisão\n",
        "y_pred_tree = tree_model.predict(X_test_scaled)\n",
        "rmse_tree = np.sqrt(mean_squared_error(y_test, y_pred_tree))\n",
        "mae_tree = mean_absolute_error(y_test, y_pred_tree)\n",
        "print(f\"Árvore de Decisão - RMSE: {rmse_tree:.4f}, MAE: {mae_tree:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Modelo 2: Random Forest Regressor\n",
        "rf_model = RandomForestRegressor(random_state=42)\n",
        "rf_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Avaliação do modelo - Random Forest\n",
        "y_pred_rf = rf_model.predict(X_test_scaled)\n",
        "rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
        "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
        "print(f\"Random Forest - RMSE: {rmse_rf:.4f}, MAE: {mae_rf:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Modelo 3: XGBoost Regressor\n",
        "xgb_model = xgb.XGBRegressor(random_state=42)\n",
        "xgb_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Avaliação do modelo - XGBoost\n",
        "y_pred_xgb = xgb_model.predict(X_test_scaled)\n",
        "rmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb))\n",
        "mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
        "print(f\"XGBoost - RMSE: {rmse_xgb:.4f}, MAE: {mae_xgb:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Passo 8: Explicabilidade com SHAP (XGBoost)\n",
        "explainer = shap.Explainer(xgb_model, X_train_scaled)\n",
        "shap_values = explainer(X_test_scaled)\n",
        "\n",
        "# Visualizando o gráfico de importância das variáveis\n",
        "shap.summary_plot(shap_values, X_test)\n",
        "\n",
        "# Passo 9: Usando o modelo XGBoost para prever no conjunto de validação\n",
        "X_validation = df_simplificado_receitas_validation.drop(columns=['correto', 'hash'])\n",
        "X_validation_scaled = scaler.transform(X_validation)\n",
        "\n",
        "y_validation_pred = xgb_model.predict(X_validation_scaled)\n",
        "\n",
        "# Visualizando as predições para o conjunto de validação\n",
        "df_simplificado_receitas_validation['predito'] = y_validation_pred\n",
        "\n",
        "# Se desejar salvar as predições para análises futuras\n",
        "df_simplificado_receitas_validation[['hash', 'correto', 'predito']].to_csv('./output/predicoes_validacao_simplificado.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TODO\n",
        "- Nos treinamentos fora dos comuns, alterar os nomes e utilizar os dataframes que foram printados ao fim do bloco \"Em Comum\"\n",
        "- Treinar o PCA com os 3 algoritmos, e criar o gráfico SHAP\n",
        "- Rede neural e processar\n",
        "- Determinante, processar e criar o gráfico SHAP"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
